## COMP3314 Asm 3 

### Background and brief analysis of dataset
In this challenge, we need to create a model that classify images into multiple classes. This dataset contains $50000$ labelled images with a dimension of $32 \times 32$. Each image is classified into one of the ten classes. While the smaller image size means that there are less dimensions to worry about, it also means that some important features might have been lost when the image size is shrunk. Therefore, we need to extract useful features in pre-processing to achieve the best result.

### Models

#### Model 1 (Based on HOG features)

#### Model 2 (Based on K-Means feature learning)

#### Model 3 (Based on Sparse Encoding and a more generalized K-Means algorithm for feature learning)

#### Feature extraction based on LBP

Further research on image classification suggests that LBP (Local Binary Pattern) is another potential candidate for object recognition. We used sklearn-image to generate the LBP features. Since LBP works on greyscale images, we first convert the images to greyscale before generating the LBP features.

The rest of the model is similar to our HOG-based model. We use LDA (Linear Discriminant Analysis) and PCA to encode the features into fewer dimensions. After that, we pass it to SVM for classification.

To fine-tune the parameters of LBP, we ran a brute-force search and used the LDA classifier to get an estimation on its final performance. Here is a diagram demonstrating the performances of the LDA classifier under different parameters.

<inserts image here>

We may notice that the accuracy of our LBP model is at around ($30\%~33%$), which is much lower than the HOG-based model. Furthermore,even after concatinating the LBP features to our training data and running SVM, we have only obtained an accuracy of $35.8%$.

If we visualize the image generated by LBP, we may notice that for some images, the outline is barely recognizable. This makes it very hard to classify the image accurately. 

<inserts image here>

This can be due to two reasons: Firstly, LBP only works on images converted to greyscale. As a result, some colour-related features have been lost when the image is converted to greyscale; Secondly, the dimensions of the images are only $32 \times 32$. This might be too low to extract useful features as the images are very blurry and the borders of some objects are not well-defined at all.

It was suggested that combining HOG and LBP will increase the accuracy of the model. Thus, we have also tested a model that uses both HOG and LBP features. However, the accuracy of the model remains about the same ($66\%$). So, based on our findings, LBP might not be a suitable descriptor in this case.

### Methodology 

#### Fine-tuning

Instead of using Grid Search, we propose to use a faster algorithm adapted from Numerical Optimization, the Golden Section Method. As we notice that using SVM, we only have one parameter to tune, which is $C$, and we hypothesis that the function $f(C)$, which gives the mean accuracy for some $C$, is concave. Hence, adapting from the Golden Section Method, we propose the following algorithm to find tune: 

Given a target precision of $\epsilon$, and an intial interval $[a_1, b_1]$, where we know the optimal hyperparameter exists in, we let $\phi = \frac{\sqrt{5}-1}{2}$, then let $$\begin{aligned} \lambda_1 &= a_1 + (1-\phi)(b-a) \\ \mu_1 &= a_1 + \phi(b-a)  \end{aligned}$$ Then, we execute the following until the target precision is reached, i.e. $b_k - a_k < \epsilon$. 
- If $f(\lambda_k) > f(\mu_k)$, then we set $a_{k+1} = a_k$, $b_{k+1} = \mu_k$, $\lambda_{k+1} = a_{k+1} + (1-\phi)(b_{k+1}-a_{k+1})$, and $\mu_{k+1} = \lambda_k$.
- Otherwise, we set $a_{k+1} = \lambda_k$, $b_{k+1} = b_k$, $\lambda_{k+1} = \mu_k$, and $\mu_{k+1} = a_{k+1} + \phi(b_{k+1}-a_{k+1})$. 

After iterating, we let $C^* = \frac{a_n+b_n}{2}$ to be the optimal parameter. As by induction, it is quite easy to show that $\frac{b_k - a_k}{b_{k+1} - a_{k+1}} = \phi$ for any $1 \leq k \leq n-1$, hence to achieve a certain accuracy $\epsilon$, $1 + \log _{\phi} \frac{\epsilon}{b_1 - a_1}$ iterations is sufficient. 

However, we acknowledge that the parameter-to-accuracy curve is not concave, especially around the maximum, hence we use this algorithm to shrink our search to a small enough interval, then perform Grid Search on that interval.