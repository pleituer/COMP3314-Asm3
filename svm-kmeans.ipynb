{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn-intelex in c:\\users\\liuho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2024.2.0)\n",
      "Requirement already satisfied: daal4py==2024.2.0 in c:\\users\\liuho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn-intelex) (2024.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\liuho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn-intelex) (1.2.2)\n",
      "Requirement already satisfied: daal==2024.2.0 in c:\\users\\liuho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from daal4py==2024.2.0->scikit-learn-intelex) (2024.2.0)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\liuho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from daal4py==2024.2.0->scikit-learn-intelex) (1.24.3)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\liuho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from daal==2024.2.0->daal4py==2024.2.0->scikit-learn-intelex) (2021.12.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\liuho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn>=0.22->scikit-learn-intelex) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\liuho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn>=0.22->scikit-learn-intelex) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\liuho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn>=0.22->scikit-learn-intelex) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn-intelex ## for x86 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import cv2\n",
    "\n",
    "from skimage.feature import hog\n",
    "from cv2 import HOGDescriptor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "## If your CPU Intel, you may use this to speed up training process\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import unpatch_sklearn\n",
    "unpatch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data and basic Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(mode=\"train\", p=0.2, seed=None):\n",
    "    \"\"\"\n",
    "    Read the training/testing files and return the input data and labels.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    mode (str): 'train' or 'test' to read the training or testing data.\n",
    "    p (float): fraction of the data to read. Useful for debugging.\n",
    "    seed (int): random seed to use for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    X (np.array): input data\n",
    "    y (np.array): labels\n",
    "    \"\"\"\n",
    "    if mode == \"train\" or mode == \"test\":\n",
    "        data = pd.read_csv('train.csv')\n",
    "        mode = \"train\"\n",
    "    elif mode == \"predict\":\n",
    "        data = pd.read_csv('test.csv')\n",
    "        mode = \"test\"\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'train' or 'test' or 'predict'\")\n",
    "\n",
    "    if mode != \"test\":\n",
    "        if seed is None:data = data.sample(frac=p)\n",
    "        else: data = data.sample(frac=p, random_state=seed)\n",
    "\n",
    "    X = data['im_name'].apply(lambda filename: np.asarray(Image.open(f\"{mode}_ims/{filename}\"))).values\n",
    "    y = data['label'].values\n",
    "    X = np.stack(X).reshape(-1, 32*32*3)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(predictedLabels, fp='test.csv'):\n",
    "    \"\"\"\n",
    "    Writes and predicts the testing files.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    predictedLabels (np.array): predicted labels for the testing data.\n",
    "    fp (str): filepath to write the predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    data = pd.read_csv('test.csv')\n",
    "    data['label'] = predictedLabels\n",
    "    data.to_csv(fp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4017, 3973, 4033, 4009, 3960, 3992, 3984, 4007, 4069, 3956]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((40000, 3072), (40000,), (10000, 3072), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData, trainLabels = read_data(\"train\", p=1, seed=69)\n",
    "trainData, testData, trainLabels, testLabels = train_test_split(trainData, trainLabels, test_size=0.2, random_state=69)\n",
    "print([sum(trainLabels == i) for i in range(10)])\n",
    "trainData.shape, trainLabels.shape, testData.shape, testLabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 3072), (10000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictData, predictLabels = read_data(\"predict\", p=1)\n",
    "predictData.shape, predictLabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog_features(data, visualize=False, type='cv2', inputShape=(32, 32, 3), dtype='int'):\n",
    "    assert type in ['sklearn', 'cv2'], \"type must be 'sklearn' or 'cv2'\"\n",
    "    if data.ndim == 1: data = data.reshape(1, -1)\n",
    "    data = data.reshape(-1, *inputShape)\n",
    "    if dtype == 'float':\n",
    "        data = ((data - data.min())/(data.max() - data.min()) * 255).astype(np.uint8)\n",
    "    if type == 'sklearn':\n",
    "        _hog_kwargs = {\n",
    "            \"block_norm\": 'L2-Hys',\n",
    "            \"orientations\": 9,\n",
    "            \"pixels_per_cell\": (8, 8),\n",
    "            \"cells_per_block\": (2, 2),\n",
    "            \"channel_axis\": -1\n",
    "        }\n",
    "        if visualize: \n",
    "            _hogs = [hog(img.reshape(32, 32, 3), **_hog_kwargs, visualize=True) for img in data]\n",
    "            _hogFeatures = np.array([_hog[0] for _hog in _hogs])\n",
    "            _hogImgs = np.array([_hog[1] for _hog in _hogs])\n",
    "            return _hogFeatures, _hogImgs\n",
    "        else:\n",
    "            return np.array([hog(img, **_hog_kwargs) for img in data])\n",
    "    else:\n",
    "        assert visualize == False, \"visualize is not supported for type='cv2'\"\n",
    "        # Following params is found to be optimal for CIFAR-10 dataset\n",
    "        _hog_kwargs = {\n",
    "            \"_winSize\": (32, 32),\n",
    "            \"_blockSize\": (12, 12),\n",
    "            \"_blockStride\": (4, 4),\n",
    "            \"_cellSize\": (4, 4),\n",
    "            \"_nbins\": 10,\n",
    "            \"_derivAperture\": 1,\n",
    "            \"_winSigma\": -1,\n",
    "            \"_histogramNormType\": 0,\n",
    "            \"_L2HysThreshold\": 0.2,\n",
    "            \"_gammaCorrection\": True,\n",
    "            \"_nlevels\": 64,\n",
    "            \"_signedGradient\": True\n",
    "        }\n",
    "        _hog = HOGDescriptor(**_hog_kwargs)\n",
    "        return np.array([_hog.compute(img) for img in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 3240), (10000, 3240), (10000, 3240))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData_hog = hog_features(trainData)\n",
    "testData_hog = hog_features(testData)\n",
    "predictData_hog = hog_features(predictData)\n",
    "trainData_hog.shape, testData_hog.shape, predictData_hog.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means and whitening Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SHAPE = (6, 6)\n",
    "def mem_str(A):\n",
    "    return str(A.shape[0]*A.shape[1]*8/(1024*1024*1024))+'GB' # Currently using float64, shd change to float32 later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchStandardize(patches):\n",
    "    \"\"\"\n",
    "    Standardizes patches by P_norm = (P - mean)/sqrt(var + 10).\n",
    "    \"\"\"\n",
    "    patches *= 255\n",
    "    patches -= patches.mean(axis=1, keepdims=True)\n",
    "    _var = patches.var(axis=1, keepdims=True) + 10\n",
    "    patches /= np.sqrt(_var)\n",
    "    return patches / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomPatches(img, patchSize, k):\n",
    "    patches = np.empty((img.shape[0] * k, patchSize[0] * patchSize[1] * 3))\n",
    "    img_samples = np.random.randint(0, img.shape[0], img.shape[0] * k)\n",
    "    row_samples = np.random.randint(0, img.shape[1] - patchSize[0] + 1, img.shape[0] * k)\n",
    "    col_samples = np.random.randint(0, img.shape[2] - patchSize[1] + 1, img.shape[0] * k)\n",
    "    for i, (imgIdx, rowIdx, colIdx) in enumerate(zip(img_samples, row_samples, col_samples)):\n",
    "        patches[i] = img[imgIdx, rowIdx:rowIdx+patchSize[0], colIdx:colIdx+patchSize[1]].ravel()\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enchanceImg(img):\n",
    "    _width = int(np.sqrt(img.shape[1] / 3))\n",
    "    img = img.reshape(_width, _width, 3)\n",
    "    return (img - img.min())/(img.max() - img.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Whiten():\n",
    "    def __init__(self, eps=1e-2):\n",
    "        self.eps = eps\n",
    "        self.mean = None\n",
    "        self.W = None\n",
    "    def zca(self, A):\n",
    "        \"\"\"\n",
    "        We aim to find a linear transformation W of X, such that the cov() on the transformed result of X will be diagonal. \n",
    "        We add two more restrictions:\n",
    "        1. The co-variance result should be close to the identity (diagnoal restricted to 1's). \n",
    "        2. W should be symetric W = W.T\n",
    "        with these restirctions, the math expression can be found at : (see the formula development at: page 48 at https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)\n",
    "        \"\"\"\n",
    "        U, S, V = np.linalg.svd(A.T @ A / A.shape[1] , full_matrices=True)\n",
    "        self.W = U @ np.diag(1.0/np.sqrt(S + self.eps)) @ U.T\n",
    "        return np.dot(A, self.W)\n",
    "    def fit_transform(self, X):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        return self.zca(X - self.mean)\n",
    "    def transform(self, X):\n",
    "        return (X - self.mean) @ self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import image\n",
    "\n",
    "class KMeansEmbedding():\n",
    "    def __init__(self, k, seed=None):\n",
    "        self.k = k\n",
    "        self.seed = seed\n",
    "        self.kmeans = MiniBatchKMeans(n_clusters=k, n_init='auto', random_state=seed)\n",
    "        self.batchSize = 1000\n",
    "        self.patchSize = PATCH_SHAPE\n",
    "        self.poolShape = (2, 2)\n",
    "        self.patchExtractor = image.PatchExtractor(patch_size=self.patchSize,random_state=seed)\n",
    "    def partial_fit(self, X):\n",
    "        self.kmeans.partial_fit(X)\n",
    "    def _transform(self, X):\n",
    "        distances = self.kmeans.transform(X)\n",
    "        embedding = np.clip(distances.mean(axis=1, keepdims=True) - distances, 0, None)\n",
    "        return embedding\n",
    "    def transform(self, X, whitener):\n",
    "        embedding = np.zeros((X.shape[0], self.poolShape[0]*self.poolShape[1]*self.k))\n",
    "        for imgIdx in range(0, X.shape[0], self.batchSize):\n",
    "            _X = X[imgIdx:imgIdx+self.batchSize]\n",
    "            poolZoneShape = ((_X.shape[1]-self.patchSize[0]+1)//self.poolShape[0], (_X.shape[2]-self.patchSize[1]+1)//self.poolShape[1])\n",
    "            zone = 0\n",
    "            for zone_x in range(0, self.poolShape[0]*poolZoneShape[0], poolZoneShape[0]):\n",
    "                for zone_y in range(0, self.poolShape[1]*poolZoneShape[1], poolZoneShape[1]):\n",
    "                    patches = self.patchExtractor.transform(_X[:, zone_x:zone_x+poolZoneShape[0]+self.patchSize[0]-1, zone_y:zone_y+poolZoneShape[1]+self.patchSize[1]-1])\n",
    "                    patches = patches.reshape(patches.shape[0], -1)\n",
    "                    patches = patchStandardize(patches)\n",
    "                    patches = whitener.transform(patches)\n",
    "                    embedding[imgIdx:imgIdx+self.batchSize, zone*self.k:(zone+1)*self.k] = self._transform(patches).reshape(_X.shape[0], int(patches.shape[0]/_X.shape[0]), self.k).sum(axis=1)\n",
    "                    zone += 1\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA_encoder():\n",
    "    def __init__(self, ndim):\n",
    "        self.ndim = ndim\n",
    "        self._lda = LinearDiscriminantAnalysis(n_components=ndim)\n",
    "        self.classCenters = None\n",
    "    def fit(self, X, y):\n",
    "        _X = self._lda.fit_transform(X, y)\n",
    "        self.classCenters = np.array([np.mean(_X[y == i], axis=0) for i in np.unique(y)])\n",
    "    def encode(self, X):\n",
    "        return 1/np.array([np.linalg.norm(self._lda.transform(X) - center, axis=1) for center in self.classCenters]).T\n",
    "    def predict(self, X, prob=False):\n",
    "        _X = self._lda.transform(X)\n",
    "        if prob:\n",
    "            return softmax(1/np.array([np.linalg.norm(_X - center, axis=1) for center in self.classCenters]).T)\n",
    "        else:\n",
    "            return np.argmin(np.array([np.linalg.norm(_X - center, axis=1) for center in self.classCenters]).T, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeansEmbedding = KMeansEmbedding(4000, seed=69)\n",
    "whitener = Whiten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of iters = 50\n",
      "Total Patches = 40000000\n",
      "Features per step per image = 20\n",
      "Each iter requires 0.6437 GB of memory\n",
      "Before Whitening\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Whitening\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/50 done\n",
      "Iteration 2/50 done\n",
      "Iteration 3/50 done\n",
      "Iteration 4/50 done\n",
      "Iteration 5/50 done\n",
      "Iteration 6/50 done\n",
      "Iteration 7/50 done\n",
      "Iteration 8/50 done\n",
      "Iteration 9/50 done\n",
      "Iteration 10/50 done\n",
      "Iteration 11/50 done\n",
      "Iteration 12/50 done\n",
      "Iteration 13/50 done\n",
      "Iteration 14/50 done\n",
      "Iteration 15/50 done\n",
      "Iteration 16/50 done\n",
      "Iteration 17/50 done\n",
      "Iteration 18/50 done\n",
      "Iteration 19/50 done\n",
      "Iteration 20/50 done\n",
      "Iteration 21/50 done\n",
      "Iteration 22/50 done\n",
      "Iteration 23/50 done\n",
      "Iteration 24/50 done\n",
      "Iteration 25/50 done\n",
      "Iteration 26/50 done\n",
      "Iteration 27/50 done\n",
      "Iteration 28/50 done\n",
      "Iteration 29/50 done\n",
      "Iteration 30/50 done\n",
      "Iteration 31/50 done\n",
      "Iteration 32/50 done\n",
      "Iteration 33/50 done\n",
      "Iteration 34/50 done\n",
      "Iteration 35/50 done\n",
      "Iteration 36/50 done\n",
      "Iteration 37/50 done\n",
      "Iteration 38/50 done\n",
      "Iteration 39/50 done\n",
      "Iteration 40/50 done\n",
      "Iteration 41/50 done\n",
      "Iteration 42/50 done\n",
      "Iteration 43/50 done\n",
      "Iteration 44/50 done\n",
      "Iteration 45/50 done\n",
      "Iteration 46/50 done\n",
      "Iteration 47/50 done\n",
      "Iteration 48/50 done\n",
      "Iteration 49/50 done\n",
      "Iteration 50/50 done\n"
     ]
    }
   ],
   "source": [
    "whitenerBatch = 40000*100//5\n",
    "\n",
    "totPatches = len(trainData) * 1000\n",
    "totSteps = int(np.ceil(totPatches/whitenerBatch))\n",
    "featuresPerStepPerImg = int(np.ceil(totPatches/totSteps/len(trainData)))\n",
    "print(f\"Num of iters = {totSteps}\\nTotal Patches = {totPatches}\\nFeatures per step per image = {featuresPerStepPerImg}\\nEach iter requires {featuresPerStepPerImg*len(trainData)*PATCH_SHAPE[0]*PATCH_SHAPE[1]*3*8/(1024*1024*1024):.4f} GB of memory\")\n",
    "for i in range(totSteps):\n",
    "    patches = getRandomPatches(trainData.reshape(-1, 32, 32, 3), PATCH_SHAPE, featuresPerStepPerImg)\n",
    "    patches = patchStandardize(patches)\n",
    "    if i == 0:\n",
    "        print(\"Before Whitening\")\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(enchanceImg(patches[:1]))\n",
    "        plt.show()\n",
    "        patches = whitener.fit_transform(patches.reshape(-1, PATCH_SHAPE[0]*PATCH_SHAPE[1]*3))\n",
    "        print(\"After Whitening\")\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(enchanceImg(patches[:1]))\n",
    "        plt.show()\n",
    "    else:\n",
    "        patches = whitener.transform(patches.reshape(-1, PATCH_SHAPE[0]*PATCH_SHAPE[1]*3))\n",
    "    kmeansEmbedding.partial_fit(patches)\n",
    "    print(f\"Iteration {i+1}/{totSteps} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 16000), (10000, 16000), (10000, 16000))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData_Embedding = kmeansEmbedding.transform(trainData.reshape(-1, 32, 32, 3), whitener)\n",
    "testData_Embedding = kmeansEmbedding.transform(testData.reshape(-1, 32, 32, 3), whitener)\n",
    "predictData_Embedding = kmeansEmbedding.transform(predictData.reshape(-1, 32, 32, 3), whitener)\n",
    "trainData_Embedding.shape, testData_Embedding.shape, predictData_Embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "np.save('kmeans4000_embeds/trainData_Embedding', trainData_Embedding)\n",
    "np.save('kmeans4000_embeds/testData_Embedding', testData_Embedding)\n",
    "np.save('kmeans4000_embeds/predictData_Embedding', predictData_Embedding)\n",
    "# and labels\n",
    "np.save('kmeans4000_embeds/trainLabels', trainLabels)\n",
    "np.save('kmeans4000_embeds/testLabels', testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA_encoder(ndim=9)\n",
    "lda.fit(trainData_Embedding, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 410), (10000, 410), (10000, 410))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData_Embedding_lda = np.hstack([trainData_Embedding, lda.encode(trainData_Embedding)])\n",
    "testData_Embedding_lda = np.hstack([testData_Embedding, lda.encode(testData_Embedding)])\n",
    "predictData_Embedding_lda = np.hstack([predictData_Embedding, lda.encode(predictData_Embedding)])\n",
    "trainData_Embedding_lda.shape, testData_Embedding_lda.shape, predictData_Embedding_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(X, p=0.2):\n",
    "    mask = np.random.rand(X.shape[0], X.shape[1]) > p\n",
    "    return X * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 3200), (10000, 3200), (10000, 3200))"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData_Embedding_masked = mask(trainData_Embedding, p=0)\n",
    "testData_Embedding_masked = mask(testData_Embedding, p=0)\n",
    "predictData_Embedding_masked = mask(predictData_Embedding, p=0)\n",
    "trainData_Embedding_masked.shape, testData_Embedding_masked.shape, predictData_Embedding_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22991468, 1.43338879, 1.77013269, 1.81065524, 0.76901093,\n",
       "       1.41512245, 0.72819399, 1.00643477, 0.86055008, 1.20136789])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData_Embedding[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from liblinear.liblinearutil import train, predict, save_model, load_model\n",
    "\n",
    "def computeTestAcc(p, trainData, trainLabels, testData, testLabels):\n",
    "    svm_ft = train(trainLabels, trainData, f'-c {10**p} -s 2')\n",
    "    return predict(testLabels, testData, svm_ft)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 68.68% (6868/10000) (classification)\n",
      "Accuracy = 68.09% (6809/10000) (classification)\n",
      "Initializations, intial interval: [-1.0000, 1.0000], error: 2.0000, optimal C: 1.0000 +/- 9.0000\n",
      "Accuracy = 68.65% (6865/10000) (classification)\n",
      "Iteration 1, interval: [-1.0000, 0.2361], error: 1.2361, optimal C: 0.4150 +/- 1.3072, accuracy: {left: 68.65, right: 68.67999999999999}\n",
      "Accuracy = 68.81% (6881/10000) (classification)\n",
      "Iteration 2, interval: [-0.5279, 0.2361], error: 0.7639, optimal C: 0.7147 +/- 1.0075, accuracy: {left: 68.67999999999999, right: 68.81}\n",
      "Accuracy = 68.46% (6846/10000) (classification)\n",
      "Iteration 3, interval: [-0.2361, 0.2361], error: 0.4721, optimal C: 1.0000 +/- 0.7221, accuracy: {left: 68.81, right: 68.46}\n",
      "Accuracy = 68.84% (6884/10000) (classification)\n",
      "Iteration 4, interval: [-0.2361, 0.0557], error: 0.2918, optimal C: 0.8125 +/- 0.3244, accuracy: {left: 68.84, right: 68.81}\n",
      "Accuracy = 69.06% (6906/10000) (classification)\n",
      "Iteration 5, interval: [-0.2361, -0.0557], error: 0.1803, optimal C: 0.7147 +/- 0.1649, accuracy: {left: 69.06, right: 68.84}\n",
      "Accuracy = 68.67% (6867/10000) (classification)\n",
      "Iteration 6, interval: [-0.2361, -0.1246], error: 0.1115, optimal C: 0.6602 +/- 0.0904, accuracy: {left: 68.67, right: 69.06}\n",
      "Accuracy = 68.65% (6865/10000) (classification)\n",
      "Iteration 7, interval: [-0.1935, -0.1246], error: 0.0689, optimal C: 0.6933 +/- 0.0572, accuracy: {left: 69.06, right: 68.65}\n",
      "Accuracy = 68.91% (6891/10000) (classification)\n",
      "Iteration 8, interval: [-0.1935, -0.1509], error: 0.0426, optimal C: 0.6727 +/- 0.0338, accuracy: {left: 68.91000000000001, right: 69.06}\n",
      "Accuracy = 68.92% (6892/10000) (classification)\n",
      "Iteration 9, interval: [-0.1772, -0.1509], error: 0.0263, optimal C: 0.6854 +/- 0.0211, accuracy: {left: 69.06, right: 68.92}\n",
      "Accuracy = 68.96% (6896/10000) (classification)\n",
      "Iteration 10, interval: [-0.1772, -0.1610], error: 0.0163, optimal C: 0.6775 +/- 0.0128, accuracy: {left: 68.96, right: 69.06}\n",
      "Accuracy = 68.42% (6842/10000) (classification)\n",
      "Iteration 11, interval: [-0.1710, -0.1610], error: 0.0100, optimal C: 0.6823 +/- 0.0079, accuracy: {left: 69.06, right: 68.42}\n",
      "Accuracy = 68.82% (6882/10000) (classification)\n",
      "Iteration 12, interval: [-0.1710, -0.1648], error: 0.0062, optimal C: 0.6793 +/- 0.0049, accuracy: {left: 68.82000000000001, right: 69.06}\n",
      "Accuracy = 68.81% (6881/10000) (classification)\n",
      "Iteration 13, interval: [-0.1687, -0.1648], error: 0.0038, optimal C: 0.6812 +/- 0.0030, accuracy: {left: 69.06, right: 68.81}\n",
      "Optimal C: 0.6812 +/- 0.0030\n"
     ]
    }
   ],
   "source": [
    "a, b = -1, 1\n",
    "precision = 5e-3\n",
    "phi = (np.sqrt(5)-1)/2\n",
    "subset = 10000\n",
    "\n",
    "l = a + (1-phi)*(b-a)\n",
    "r = a + phi*(b-a)\n",
    "fl = computeTestAcc(l, trainData_Embedding[:subset], trainLabels[:subset], testData_Embedding, testLabels)\n",
    "fr = computeTestAcc(r, trainData_Embedding[:subset], trainLabels[:subset], testData_Embedding, testLabels)\n",
    "\n",
    "iter = 0\n",
    "\n",
    "print(f\"Initializations, intial interval: [{a:.4f}, {b:.4f}], error: {b-a:.4f}, optimal C: {10**((b+a)/2):.4f} +/- {10**b-10**((b+a)/2):.4f}\")\n",
    "\n",
    "while b - a > precision:\n",
    "    if fl < fr:\n",
    "        a = l\n",
    "        l = r\n",
    "        fl = fr\n",
    "        r = a + phi*(b-a)\n",
    "        fr = computeTestAcc(r, trainData_Embedding[:subset], trainLabels[:subset], testData_Embedding, testLabels)\n",
    "    else:\n",
    "        b = r\n",
    "        r = l\n",
    "        fr = fl\n",
    "        l = a + (1-phi)*(b-a)\n",
    "        fl = computeTestAcc(l, trainData_Embedding[:subset], trainLabels[:subset], testData_Embedding, testLabels)\n",
    "    iter += 1\n",
    "    print(f\"Iteration {iter}, interval: [{a:.4f}, {b:.4f}], error: {b-a:.4f}, optimal C: {10**((b+a)/2):.4f} +/- {10**b-10**((b+a)/2):.4f}, accuracy: {{left: {fl}, right: {fr}}}\")\n",
    "\n",
    "optimalC = 10**((b+a)/2)\n",
    "print(f\"Optimal C: {optimalC:.4f} +/- {10**b-optimalC:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = train(trainLabels, trainData_Embedding, f'-c {optimalC} -s 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 74.3% (7430/10000) (classification)\n"
     ]
    }
   ],
   "source": [
    "test_pred, test_acc, test_val = predict(testLabels, testData_Embedding, svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-16 {color: black;background-color: white;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(C=0.11755551787634247, dual=False, random_state=69)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=0.11755551787634247, dual=False, random_state=69)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(C=0.11755551787634247, dual=False, random_state=69)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm = svm.LinearSVC(C=optimalC, dual=False, random_state=69)\n",
    "svm.fit(trainData_Embedding, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.749"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = svm.predict(testData_Embedding_masked)\n",
    "(test_pred == testLabels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8325"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# erm slow\n",
    "train_pred = svm.predict(trainData_Embedding_masked)\n",
    "(train_pred == trainLabels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(testLabels, \u001b[43mtest_pred\u001b[49m)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(cm, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_pred' is not defined"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(testLabels, test_pred)\n",
    "plt.imshow(cm, cmap='Blues')\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center', color='black' if cm[i, j] < cm.max()/2 else 'white')\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.lr = nn.Linear(input_dim, 10)\n",
    "    def forward(self, X):\n",
    "        X = self.lr(X)\n",
    "        return X\n",
    "    def train(self, trainData, trainLabels, testData, testLabels, mode='iterate'):\n",
    "        epochs = 500\n",
    "        optimizer = optim.Adam(self.parameters(), lr=7e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        trainData_dataset = data.TensorDataset(trainData, trainLabels)\n",
    "        trainData_dataloader = data.DataLoader(trainData_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            tot_loss = 0\n",
    "            for X, y in trainData_dataloader:\n",
    "                outputs = self(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                tot_loss += loss.item()\n",
    "            tot_loss /= len(trainData_dataloader)\n",
    "            if (epoch+1) % 5 == 0:\n",
    "                train_acc = (self(trainData).argmax(axis=1) == trainLabels).to(dtype=torch.float32).mean().item()\n",
    "                test_acc = (self(testData).argmax(axis=1) == testLabels).to(dtype=torch.float32).mean().item()\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {tot_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "            if epochs == 100:\n",
    "                for param in optimizer.param_groups:\n",
    "                    param['lr'] = 1e-4\n",
    "            if epochs == 200:\n",
    "                for param in optimizer.param_groups:\n",
    "                    param['lr'] = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(trainData_Embedding.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([40000, 3200]),\n",
       " torch.Size([40000]),\n",
       " torch.Size([10000, 3200]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData_Embedding_tensor = torch.tensor(trainData_Embedding, dtype=torch.float32)\n",
    "trainLabels_tensor = torch.tensor(trainLabels, dtype=torch.long)\n",
    "testData_Embedding_tensor = torch.tensor(testData_Embedding, dtype=torch.float32)\n",
    "testLabels_tensor = torch.tensor(testLabels, dtype=torch.long)\n",
    "trainData_Embedding_tensor.shape, trainLabels_tensor.shape, testData_Embedding_tensor.shape, testLabels_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/500, Loss: 1.1179, Train Acc: 0.6227, Test Acc: 0.6052\n",
      "Epoch 10/500, Loss: 1.0368, Train Acc: 0.6472, Test Acc: 0.6300\n",
      "Epoch 15/500, Loss: 0.9950, Train Acc: 0.6599, Test Acc: 0.6346\n",
      "Epoch 20/500, Loss: 0.9662, Train Acc: 0.6475, Test Acc: 0.6262\n",
      "Epoch 25/500, Loss: 0.9391, Train Acc: 0.6732, Test Acc: 0.6496\n",
      "Epoch 30/500, Loss: 0.9190, Train Acc: 0.6940, Test Acc: 0.6638\n",
      "Epoch 35/500, Loss: 0.9012, Train Acc: 0.6963, Test Acc: 0.6701\n",
      "Epoch 40/500, Loss: 0.8967, Train Acc: 0.6996, Test Acc: 0.6686\n",
      "Epoch 45/500, Loss: 0.8801, Train Acc: 0.7034, Test Acc: 0.6704\n",
      "Epoch 50/500, Loss: 0.8772, Train Acc: 0.7054, Test Acc: 0.6733\n",
      "Epoch 55/500, Loss: 0.8606, Train Acc: 0.7068, Test Acc: 0.6734\n",
      "Epoch 60/500, Loss: 0.8487, Train Acc: 0.7067, Test Acc: 0.6812\n",
      "Epoch 65/500, Loss: 0.8363, Train Acc: 0.7138, Test Acc: 0.6825\n",
      "Epoch 70/500, Loss: 0.8351, Train Acc: 0.7103, Test Acc: 0.6799\n",
      "Epoch 75/500, Loss: 0.8294, Train Acc: 0.7121, Test Acc: 0.6763\n",
      "Epoch 80/500, Loss: 0.8157, Train Acc: 0.7163, Test Acc: 0.6809\n",
      "Epoch 85/500, Loss: 0.8103, Train Acc: 0.7184, Test Acc: 0.6820\n",
      "Epoch 90/500, Loss: 0.8059, Train Acc: 0.7254, Test Acc: 0.6875\n",
      "Epoch 95/500, Loss: 0.8022, Train Acc: 0.7217, Test Acc: 0.6843\n",
      "Epoch 100/500, Loss: 0.7980, Train Acc: 0.7344, Test Acc: 0.6939\n",
      "Epoch 105/500, Loss: 0.7959, Train Acc: 0.7330, Test Acc: 0.6916\n",
      "Epoch 110/500, Loss: 0.7872, Train Acc: 0.7410, Test Acc: 0.7002\n",
      "Epoch 115/500, Loss: 0.7833, Train Acc: 0.7295, Test Acc: 0.6897\n",
      "Epoch 120/500, Loss: 0.7794, Train Acc: 0.7446, Test Acc: 0.7033\n",
      "Epoch 125/500, Loss: 0.7695, Train Acc: 0.7365, Test Acc: 0.6926\n",
      "Epoch 130/500, Loss: 0.7729, Train Acc: 0.7299, Test Acc: 0.6893\n",
      "Epoch 135/500, Loss: 0.7670, Train Acc: 0.7491, Test Acc: 0.7076\n",
      "Epoch 140/500, Loss: 0.7682, Train Acc: 0.7388, Test Acc: 0.6959\n",
      "Epoch 145/500, Loss: 0.7586, Train Acc: 0.7307, Test Acc: 0.6885\n",
      "Epoch 150/500, Loss: 0.7468, Train Acc: 0.7520, Test Acc: 0.7091\n",
      "Epoch 155/500, Loss: 0.7538, Train Acc: 0.7365, Test Acc: 0.6967\n",
      "Epoch 160/500, Loss: 0.7538, Train Acc: 0.7485, Test Acc: 0.7026\n",
      "Epoch 165/500, Loss: 0.7496, Train Acc: 0.7464, Test Acc: 0.7027\n",
      "Epoch 170/500, Loss: 0.7437, Train Acc: 0.7444, Test Acc: 0.7002\n",
      "Epoch 175/500, Loss: 0.7447, Train Acc: 0.7531, Test Acc: 0.7057\n",
      "Epoch 180/500, Loss: 0.7372, Train Acc: 0.7539, Test Acc: 0.7034\n",
      "Epoch 185/500, Loss: 0.7373, Train Acc: 0.7567, Test Acc: 0.7096\n",
      "Epoch 190/500, Loss: 0.7320, Train Acc: 0.7511, Test Acc: 0.6996\n",
      "Epoch 195/500, Loss: 0.7291, Train Acc: 0.7661, Test Acc: 0.7166\n",
      "Epoch 200/500, Loss: 0.7286, Train Acc: 0.7531, Test Acc: 0.7014\n",
      "Epoch 205/500, Loss: 0.7219, Train Acc: 0.7571, Test Acc: 0.7105\n",
      "Epoch 210/500, Loss: 0.7245, Train Acc: 0.7691, Test Acc: 0.7190\n",
      "Epoch 215/500, Loss: 0.7185, Train Acc: 0.7670, Test Acc: 0.7165\n",
      "Epoch 220/500, Loss: 0.7160, Train Acc: 0.7643, Test Acc: 0.7115\n",
      "Epoch 225/500, Loss: 0.7193, Train Acc: 0.7627, Test Acc: 0.7107\n",
      "Epoch 230/500, Loss: 0.7096, Train Acc: 0.7627, Test Acc: 0.7110\n",
      "Epoch 235/500, Loss: 0.7072, Train Acc: 0.7622, Test Acc: 0.7076\n",
      "Epoch 240/500, Loss: 0.7021, Train Acc: 0.7577, Test Acc: 0.7103\n",
      "Epoch 245/500, Loss: 0.7055, Train Acc: 0.7642, Test Acc: 0.7096\n",
      "Epoch 250/500, Loss: 0.7057, Train Acc: 0.7660, Test Acc: 0.7120\n",
      "Epoch 255/500, Loss: 0.6970, Train Acc: 0.7597, Test Acc: 0.7079\n",
      "Epoch 260/500, Loss: 0.7031, Train Acc: 0.7418, Test Acc: 0.6922\n",
      "Epoch 265/500, Loss: 0.6966, Train Acc: 0.7737, Test Acc: 0.7149\n",
      "Epoch 270/500, Loss: 0.6930, Train Acc: 0.7710, Test Acc: 0.7149\n",
      "Epoch 275/500, Loss: 0.6916, Train Acc: 0.7743, Test Acc: 0.7187\n",
      "Epoch 280/500, Loss: 0.6966, Train Acc: 0.7661, Test Acc: 0.7127\n",
      "Epoch 285/500, Loss: 0.6961, Train Acc: 0.7538, Test Acc: 0.7017\n",
      "Epoch 290/500, Loss: 0.6960, Train Acc: 0.7710, Test Acc: 0.7122\n",
      "Epoch 295/500, Loss: 0.6900, Train Acc: 0.7707, Test Acc: 0.7139\n",
      "Epoch 300/500, Loss: 0.6828, Train Acc: 0.7747, Test Acc: 0.7194\n",
      "Epoch 305/500, Loss: 0.6830, Train Acc: 0.7582, Test Acc: 0.7020\n",
      "Epoch 310/500, Loss: 0.6801, Train Acc: 0.7581, Test Acc: 0.7079\n",
      "Epoch 315/500, Loss: 0.6858, Train Acc: 0.7460, Test Acc: 0.6966\n",
      "Epoch 320/500, Loss: 0.6790, Train Acc: 0.7637, Test Acc: 0.7056\n",
      "Epoch 325/500, Loss: 0.6795, Train Acc: 0.7654, Test Acc: 0.7110\n",
      "Epoch 330/500, Loss: 0.6790, Train Acc: 0.7782, Test Acc: 0.7189\n",
      "Epoch 335/500, Loss: 0.6733, Train Acc: 0.7852, Test Acc: 0.7229\n",
      "Epoch 340/500, Loss: 0.6750, Train Acc: 0.7666, Test Acc: 0.7078\n",
      "Epoch 345/500, Loss: 0.6699, Train Acc: 0.7739, Test Acc: 0.7140\n",
      "Epoch 350/500, Loss: 0.6743, Train Acc: 0.7688, Test Acc: 0.7109\n",
      "Epoch 355/500, Loss: 0.6691, Train Acc: 0.7793, Test Acc: 0.7196\n",
      "Epoch 360/500, Loss: 0.6655, Train Acc: 0.7700, Test Acc: 0.7130\n",
      "Epoch 365/500, Loss: 0.6674, Train Acc: 0.7793, Test Acc: 0.7167\n",
      "Epoch 370/500, Loss: 0.6681, Train Acc: 0.7823, Test Acc: 0.7249\n",
      "Epoch 375/500, Loss: 0.6570, Train Acc: 0.7869, Test Acc: 0.7237\n",
      "Epoch 380/500, Loss: 0.6614, Train Acc: 0.7646, Test Acc: 0.7070\n",
      "Epoch 385/500, Loss: 0.6628, Train Acc: 0.7894, Test Acc: 0.7241\n",
      "Epoch 390/500, Loss: 0.6622, Train Acc: 0.7688, Test Acc: 0.7107\n",
      "Epoch 395/500, Loss: 0.6562, Train Acc: 0.7797, Test Acc: 0.7207\n",
      "Epoch 400/500, Loss: 0.6565, Train Acc: 0.7842, Test Acc: 0.7223\n",
      "Epoch 405/500, Loss: 0.6552, Train Acc: 0.7848, Test Acc: 0.7228\n",
      "Epoch 410/500, Loss: 0.6555, Train Acc: 0.7666, Test Acc: 0.7050\n",
      "Epoch 415/500, Loss: 0.6485, Train Acc: 0.7778, Test Acc: 0.7182\n",
      "Epoch 420/500, Loss: 0.6507, Train Acc: 0.7829, Test Acc: 0.7170\n",
      "Epoch 425/500, Loss: 0.6595, Train Acc: 0.7778, Test Acc: 0.7115\n",
      "Epoch 430/500, Loss: 0.6543, Train Acc: 0.7806, Test Acc: 0.7154\n",
      "Epoch 435/500, Loss: 0.6494, Train Acc: 0.7904, Test Acc: 0.7226\n",
      "Epoch 440/500, Loss: 0.6477, Train Acc: 0.7741, Test Acc: 0.7121\n",
      "Epoch 445/500, Loss: 0.6485, Train Acc: 0.7734, Test Acc: 0.7131\n",
      "Epoch 450/500, Loss: 0.6442, Train Acc: 0.7930, Test Acc: 0.7240\n",
      "Epoch 455/500, Loss: 0.6439, Train Acc: 0.7626, Test Acc: 0.7052\n",
      "Epoch 460/500, Loss: 0.6425, Train Acc: 0.7818, Test Acc: 0.7178\n",
      "Epoch 465/500, Loss: 0.6413, Train Acc: 0.7876, Test Acc: 0.7197\n",
      "Epoch 470/500, Loss: 0.6360, Train Acc: 0.7807, Test Acc: 0.7146\n",
      "Epoch 475/500, Loss: 0.6423, Train Acc: 0.7698, Test Acc: 0.7048\n",
      "Epoch 480/500, Loss: 0.6386, Train Acc: 0.7656, Test Acc: 0.6995\n",
      "Epoch 485/500, Loss: 0.6334, Train Acc: 0.7890, Test Acc: 0.7192\n",
      "Epoch 490/500, Loss: 0.6339, Train Acc: 0.7700, Test Acc: 0.7078\n",
      "Epoch 495/500, Loss: 0.6332, Train Acc: 0.7884, Test Acc: 0.7219\n",
      "Epoch 500/500, Loss: 0.6382, Train Acc: 0.7891, Test Acc: 0.7184\n"
     ]
    }
   ],
   "source": [
    "lr.train(trainData_Embedding_tensor, trainLabels_tensor, testData_Embedding_tensor, testLabels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
